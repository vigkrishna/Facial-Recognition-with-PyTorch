{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSAKOrzCpEoBWP95MMJWrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3b22786fe54460da76dc4ef782a0d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_796a82918b53497a98083999f472f5d3",
              "IPY_MODEL_4c591cdbb60e42bfb9eebf7bf111292b",
              "IPY_MODEL_3cf3847b81b849a09d1f464fabe20c22"
            ],
            "layout": "IPY_MODEL_ad55774e44a045ac9afdc35c6e7a3cc3"
          }
        },
        "796a82918b53497a98083999f472f5d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5393f114890a4543bcaad84327671744",
            "placeholder": "​",
            "style": "IPY_MODEL_8c9e53cb7f6f44f19daf270ba2859f3f",
            "value": "model.safetensors: 100%"
          }
        },
        "4c591cdbb60e42bfb9eebf7bf111292b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac5c95ad9da1485e9636628f4b93e1a8",
            "max": 21355344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82ff0944330f478382bc7a59d1dfdc40",
            "value": 21355344
          }
        },
        "3cf3847b81b849a09d1f464fabe20c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ece5e33ae20421f948e6e4ab640323c",
            "placeholder": "​",
            "style": "IPY_MODEL_ff69775026824bd4ba41c3a04104522a",
            "value": " 21.4M/21.4M [00:00&lt;00:00, 42.9MB/s]"
          }
        },
        "ad55774e44a045ac9afdc35c6e7a3cc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5393f114890a4543bcaad84327671744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c9e53cb7f6f44f19daf270ba2859f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac5c95ad9da1485e9636628f4b93e1a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82ff0944330f478382bc7a59d1dfdc40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ece5e33ae20421f948e6e4ab640323c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff69775026824bd4ba41c3a04104522a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vigkrishna/Facial-Recognition-with-PyTorch/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8qggsS3YCQ-",
        "outputId": "63e95c26-118f-4ecb-a683-37902df03711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Facial-Expression-Dataset'...\n",
            "remote: Enumerating objects: 34052, done.\u001b[K\n",
            "remote: Total 34052 (delta 0), reused 0 (delta 0), pack-reused 34052 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34052/34052), 52.31 MiB | 21.75 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "Updating files: 100% (35887/35887), done.\n",
            "Collecting git+https://github.com/albumentations-team/albumentations\n",
            "  Cloning https://github.com/albumentations-team/albumentations to /tmp/pip-req-build-h3oj1jtq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/albumentations-team/albumentations /tmp/pip-req-build-h3oj1jtq\n",
            "  Resolved https://github.com/albumentations-team/albumentations to commit 2bc1c14bc14463df085145fc9e632a0256adc4f9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.0) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.0) (2.10.5)\n",
            "Collecting albucore==0.0.23 (from albumentations==2.0.0)\n",
            "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.0) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations==2.0.0) (3.11.3)\n",
            "Collecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations==2.0.0)\n",
            "  Downloading simsimd-6.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations==2.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations==2.0.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations==2.0.0) (4.12.2)\n",
            "Downloading albucore-0.0.23-py3-none-any.whl (14 kB)\n",
            "Downloading simsimd-6.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (632 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-2.0.0-py3-none-any.whl size=274955 sha256=404c466be0069f725a1559d430521d39b1ebba86e8c055f28802e36355e083dd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2p9olbkz/wheels/d8/87/c6/794399113ca308f93f3af45afd7d85eff07615a89a2b799e91\n",
            "Successfully built albumentations\n",
            "Installing collected packages: simsimd, albucore, albumentations\n",
            "  Attempting uninstall: albucore\n",
            "    Found existing installation: albucore 0.0.19\n",
            "    Uninstalling albucore-0.0.19:\n",
            "      Successfully uninstalled albucore-0.0.19\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.4.20\n",
            "    Uninstalling albumentations-1.4.20:\n",
            "      Successfully uninstalled albumentations-1.4.20\n",
            "Successfully installed albucore-0.0.23 albumentations-2.0.0 simsimd-6.2.1\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.13)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.27.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Collecting opencv-contrib-python\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-contrib-python) (1.26.4)\n",
            "Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-contrib-python\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.10.0.84\n",
            "    Uninstalling opencv-contrib-python-4.10.0.84:\n",
            "      Successfully uninstalled opencv-contrib-python-4.10.0.84\n",
            "Successfully installed opencv-contrib-python-4.11.0.86\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/parth1620/Facial-Expression-Dataset.git\n",
        "!pip install -U git+https://github.com/albumentations-team/albumentations\n",
        "!pip install timm\n",
        "!pip install --upgrade opencv-contrib-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTS --------**"
      ],
      "metadata": {
        "id": "eLtxw02gZz1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ],
      "metadata": {
        "id": "Ylr9vvp9YdL0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONFIGURATIONS ------**"
      ],
      "metadata": {
        "id": "OaNKS5hoZsSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jonathanoheix/face-expression-recognition-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AmjQPNMas-7",
        "outputId": "ca4c9bdc-4d0a-4591-cf65-864148aa5f47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jonathanoheix/face-expression-recognition-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 121M/121M [00:03<00:00, 33.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_IMG_FOLDER_PATH = '/content/Facial-Expression-Dataset/train'\n",
        "VALID_IMG_FOLDER_PATH = '/content/Facial-Expression-Dataset/validation'\n",
        "\n",
        "\n",
        "lr = 0.01\n",
        "BATCH_SIZE=32\n",
        "epochs =15\n",
        "\n",
        "device ='cuda'\n",
        "model_name = 'efficientnet_b0'"
      ],
      "metadata": {
        "id": "yGZ3TXbuZqdx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms as T"
      ],
      "metadata": {
        "id": "haXCE2LqYvdO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_augs = T.Compose([\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(degrees=(-20,+20)),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "valid_augs = T.Compose([\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "Gkr89S53ZktS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = ImageFolder(TRAIN_IMG_FOLDER_PATH,transform = train_augs)\n",
        "validset = ImageFolder(VALID_IMG_FOLDER_PATH,transform = valid_augs)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Total no. of examples in trainset : {len(trainset)}\")\n",
        "print(f\"Total no. of examples in validset : {len(validset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGukd50kYz02",
        "outputId": "dad0e792-9ffc-4302-cf0f-5fef830a50c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of examples in trainset : 28821\n",
            "Total no. of examples in validset : 7066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image,label = trainset[3000]\n",
        "\n",
        "plt.imshow(image.permute(1,2,0))  # (c,w,h) format\n",
        "plt.title(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "gF5Xj1kTY4I-",
        "outputId": "7e486a60-8c21-435a-9a35-6dac083f80ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '0')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainset.class_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkWqiZiIhWjX",
        "outputId": "79f71a8b-04bf-48a1-c275-d5eda7fdc2a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Dataset into Batches**"
      ],
      "metadata": {
        "id": "j8WUV_yGiPyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "eLhIUc84hxRx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = T)\n",
        "validloader = DataLoader(validset, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "LLlGhZPsiZP2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the model**"
      ],
      "metadata": {
        "id": "hhYeDcanmIlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import timm\n",
        "from torch import nn\n",
        "\n",
        "class FERModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FERModel, self).__init__()\n",
        "        self.eff_net = timm.create_model('efficientnet_b0', pretrained=True,num_classes=7)\n",
        "\n",
        "\n",
        "    def forward(self, images,labels=None):\n",
        "        logits = self.eff_net(images)\n",
        "\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "            return logits,loss\n",
        "\n",
        "        return logits\n",
        "\n",
        "model = FERModel()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "e3b22786fe54460da76dc4ef782a0d69",
            "796a82918b53497a98083999f472f5d3",
            "4c591cdbb60e42bfb9eebf7bf111292b",
            "3cf3847b81b849a09d1f464fabe20c22",
            "ad55774e44a045ac9afdc35c6e7a3cc3",
            "5393f114890a4543bcaad84327671744",
            "8c9e53cb7f6f44f19daf270ba2859f3f",
            "ac5c95ad9da1485e9636628f4b93e1a8",
            "82ff0944330f478382bc7a59d1dfdc40",
            "9ece5e33ae20421f948e6e4ab640323c",
            "ff69775026824bd4ba41c3a04104522a"
          ]
        },
        "id": "8w7QlilKkoFE",
        "outputId": "8ba5ad1e-975e-4a76-9dc5-1c8bcffa4a99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3b22786fe54460da76dc4ef782a0d69"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Train and Eval Function**"
      ],
      "metadata": {
        "id": "Sl5JDIw_9DnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddIhoUeO9UIu",
        "outputId": "003e1c8c-f918-42d7-a76d-9e849562bf6e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tqdn\n",
            "  Downloading tqdn-0.2.1-py3-none-any.whl.metadata (688 bytes)\n",
            "Downloading tqdn-0.2.1-py3-none-any.whl (2.9 kB)\n",
            "Installing collected packages: tqdn\n",
            "Successfully installed tqdn-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Q345fwXJ9DMu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_accuracy(y_pred,y_true):\n",
        "  top_p,top_class = y_pred.topk(1,dim=1)\n",
        "  equals = top_class == y_true.view(*top_class.shape)\n",
        "  return torch.mean(equals.type(torch.FloatTensor))"
      ],
      "metadata": {
        "id": "pjElEf3j9XTJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model,dataloader,optimizer,current_epo):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  total_loss=0\n",
        "  total_acc=0\n",
        "  tk = tqdm(dataloader,desc = \"epoc\" + \"{TRAIN}\" + str(current_epo + 1) + \"/\" + str(epochs))\n",
        "\n",
        "\n",
        "  for t, data in enumerate(tk):\n",
        "    images,labels = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits,loss = model(images,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    total_acc += multiclass_accuracy(logits,labels)\n",
        "    tk.set_postfix({'loss' : loss.item(),'acc' : multiclass_accuracy(logits,labels)})\n",
        "\n",
        "\n",
        "  return total_loss / len(dataloader),total_acc / len(dataloader)\n",
        ""
      ],
      "metadata": {
        "id": "eatZjzho9yCr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_fn(model,dataloader,optimizer,current_epo):\n",
        "  model.eval()\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  total_loss=0\n",
        "  total_acc=0\n",
        "  tk = tqdm(dataloader,desc = \"epoc\" + \"{VALID}\" + str(current_epo + 1) + \"/\" + str(epochs))\n",
        "\n",
        "\n",
        "  for t, data in enumerate(tk):\n",
        "    images,labels = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "\n",
        "    logits,loss = model(images,labels)\n",
        "\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    total_acc += multiclass_accuracy(logits,labels)\n",
        "    tk.set_postfix({'loss' : loss.item(),'acc' : multiclass_accuracy(logits,labels)})\n",
        "\n",
        "\n",
        "  return total_loss / len(dataloader),total_acc / len(dataloader)"
      ],
      "metadata": {
        "id": "Fpe0DnwA-faZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATING THE TRAINING LOOP**"
      ],
      "metadata": {
        "id": "lIR7Lmn-AHLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr = lr)"
      ],
      "metadata": {
        "id": "12TfRhiWAMd7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_valid_loss = np.Inf\n",
        "\n",
        "for i in range(epochs):\n",
        "    train_loss, train_acc = train_fn(model, trainloader, optimizer, i)  # Pass model, not model_name\n",
        "    valid_loss, valid_acc = eval_fn(model, validloader, optimizer, i)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best_model_weights.pt')\n",
        "        print(\"Saved model weights\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVY8TWC-AiNm",
        "outputId": "78f3a849-3d0c-424c-af29-4672870effd6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}1/15: 100%|██████████| 901/901 [00:57<00:00, 15.77it/s, loss=1.17, acc=tensor(0.6667)]\n",
            "epoc{VALID}1/15: 100%|██████████| 221/221 [00:06<00:00, 32.19it/s, loss=3.31, acc=tensor(0.5385)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}2/15: 100%|██████████| 901/901 [00:49<00:00, 18.33it/s, loss=1.85, acc=tensor(0.2857)]\n",
            "epoc{VALID}2/15: 100%|██████████| 221/221 [00:06<00:00, 35.32it/s, loss=1.24, acc=tensor(0.6154)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}3/15: 100%|██████████| 901/901 [00:48<00:00, 18.76it/s, loss=1.23, acc=tensor(0.4762)]\n",
            "epoc{VALID}3/15: 100%|██████████| 221/221 [00:06<00:00, 34.78it/s, loss=0.874, acc=tensor(0.6923)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}4/15: 100%|██████████| 901/901 [00:47<00:00, 18.98it/s, loss=1.5, acc=tensor(0.3333)]\n",
            "epoc{VALID}4/15: 100%|██████████| 221/221 [00:06<00:00, 32.05it/s, loss=1, acc=tensor(0.6923)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}5/15: 100%|██████████| 901/901 [00:47<00:00, 18.80it/s, loss=1.15, acc=tensor(0.5714)]\n",
            "epoc{VALID}5/15: 100%|██████████| 221/221 [00:06<00:00, 36.08it/s, loss=0.743, acc=tensor(0.7692)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}6/15: 100%|██████████| 901/901 [00:53<00:00, 16.71it/s, loss=0.96, acc=tensor(0.5714)]\n",
            "epoc{VALID}6/15: 100%|██████████| 221/221 [00:06<00:00, 33.64it/s, loss=0.62, acc=tensor(0.8462)]\n",
            "epoc{TRAIN}7/15: 100%|██████████| 901/901 [00:48<00:00, 18.65it/s, loss=1.65, acc=tensor(0.3333)]\n",
            "epoc{VALID}7/15: 100%|██████████| 221/221 [00:06<00:00, 35.72it/s, loss=1.65, acc=tensor(0.3846)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}8/15: 100%|██████████| 901/901 [00:48<00:00, 18.41it/s, loss=1.9, acc=tensor(0.3333)]\n",
            "epoc{VALID}8/15: 100%|██████████| 221/221 [00:06<00:00, 32.34it/s, loss=0.587, acc=tensor(0.8846)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}9/15: 100%|██████████| 901/901 [00:48<00:00, 18.76it/s, loss=1.12, acc=tensor(0.5714)]\n",
            "epoc{VALID}9/15: 100%|██████████| 221/221 [00:06<00:00, 35.99it/s, loss=0.466, acc=tensor(0.8846)]\n",
            "epoc{TRAIN}10/15: 100%|██████████| 901/901 [00:48<00:00, 18.70it/s, loss=1.21, acc=tensor(0.5238)]\n",
            "epoc{VALID}10/15: 100%|██████████| 221/221 [00:06<00:00, 32.47it/s, loss=1.22, acc=tensor(0.5385)]\n",
            "epoc{TRAIN}11/15: 100%|██████████| 901/901 [00:47<00:00, 18.99it/s, loss=0.747, acc=tensor(0.8095)]\n",
            "epoc{VALID}11/15: 100%|██████████| 221/221 [00:06<00:00, 33.36it/s, loss=1.6, acc=tensor(0.5000)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}12/15: 100%|██████████| 901/901 [00:48<00:00, 18.74it/s, loss=0.643, acc=tensor(0.7143)]\n",
            "epoc{VALID}12/15: 100%|██████████| 221/221 [00:06<00:00, 35.93it/s, loss=0.746, acc=tensor(0.6923)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}13/15: 100%|██████████| 901/901 [00:48<00:00, 18.74it/s, loss=1.09, acc=tensor(0.6667)]\n",
            "epoc{VALID}13/15: 100%|██████████| 221/221 [00:06<00:00, 32.12it/s, loss=0.771, acc=tensor(0.7692)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}14/15: 100%|██████████| 901/901 [00:47<00:00, 18.84it/s, loss=1.24, acc=tensor(0.6190)]\n",
            "epoc{VALID}14/15: 100%|██████████| 221/221 [00:06<00:00, 35.48it/s, loss=0.703, acc=tensor(0.7692)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoc{TRAIN}15/15: 100%|██████████| 901/901 [00:47<00:00, 18.82it/s, loss=0.961, acc=tensor(0.6667)]\n",
            "epoc{VALID}15/15: 100%|██████████| 221/221 [00:06<00:00, 32.81it/s, loss=0.744, acc=tensor(0.6923)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE**"
      ],
      "metadata": {
        "id": "5KDjEJWniKcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "model = FERModel()\n",
        "model.load_state_dict(torch.load('best_model_weights.pt'))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "inference_transforms = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "def predict_image(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image = inference_transforms(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(image)\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "\n",
        "    class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "    predicted_class_name = class_names[predicted_class]\n",
        "\n",
        "    return predicted_class_name, probabilities\n",
        "\n",
        "\n",
        "image_path = 'img_path'\n",
        "predicted_label, probabilities = predict_image(image_path)\n",
        "\n",
        "print(f\"Predicted Class: {predicted_label}\")\n",
        "print(f\"Probabilities: {probabilities}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSFolIK2BHUa",
        "outputId": "7f65a40d-b26e-479d-b725-bff753082574"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-8c4dd15f1deb>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model_weights.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: Angry\n",
            "Probabilities: tensor([[9.3120e-01, 1.3073e-06, 1.3935e-04, 4.4857e-02, 1.9223e-03, 2.1868e-02,\n",
            "         1.2058e-05]], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}